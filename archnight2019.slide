秒間数千万メトリクスを裁く監視基盤のアーキテクチャ

huydx
エンジニア/LINE Corp
@dxhuy

* 今日の話
- 弊社のObservability事情
- 時系列データベースを作ったの楽しい話
- 時系列データベースを作ったの辛い話


* LINEのObservabilityチーム

- LINE自体ができる前から作っていたモニタリングプラットフォーム
- 社内全体にプラットフォーム（SASS)として提供している 
- Metrics / Log / Alert / Distributed Tracingなどをたくさん機能を提供する

* Observabilityの三つの柱

* 今日はMetrics柱をフォーカスします


* メトリクスプラットフォームを提供するためには

- Diagram to explain what module we need

* メトリクスストレージ概要

- データポイントを保存する
- レンジを指定してデータポイントをクエリする


* メトリクスストレージ用語

- image


* メトリクスストレージ概要
.code ./code/metrics1.go  


* 弊社のメトリクスストレージ歴史
- ~ 2016 Mysqlでめっちゃ頑張った
- 2016 ~ 2018　OpenTSDBに切り替えた
- 2018 OpenTSDBの限界を感じて新しいストレージ開発を着手始めた
- 2019 新しいストレージをリリース

* 僕たちが求められるワークロード
- １分間：数億メトリクス以上さばける
- 秒間数千クエリ捌ける

* なぜオープンソース利用しないの？
- table image all OSS

* なぜDatadog,Mackerelなど利用しないの？
- マイグレーションコスト
- 利用コスト
- カスタマイズ問題（例えば：年末年始のメトリクスは永久保存, 細かいACL制御）

* 結果: ツールの限界を超えて、自分たちで作るのを決めた

* 成果

- image


* 本題：どうやって作れる？

* 基本のアイディア
- 機能ごとにマイクロサービスに分散して最適化する
- できるだけメモリにフィットさせる
- できる部分をオープンソースを使って開発コストを下げる

* ざっくりアーキテクチャ

- image

* コミュニケーションプロトコルはGrpc
- Grpcは早くはない（marshal/unmarshal）は遅いとメモリアロケーションが爆発
- だが、便利の機能がたくさん揃っている（クライアントサイドLB,自由にカスタマイズできるResolver,リトライなど）

* メモリベースストレージの難しさ

- スケールアウトするためにステート持つのがだめ
- First Class アプリケーション(気軽にデプロイ、再起動)として作りたい
  - 再起動したらデータロスはだめ
  - ホストが死んだらデータロスも絶対ダメ

* スケールアウト問題

- Hashベースのデータ分散
- image


* ゼロデータロス問題（１）


* ゼロデータロス問題（２）


* RAFTベースのクラスター構成 


* データの永久保存

- 直接メモリからバッチサーバでストリーミングする
- ストリーミング単位はできるだけ大きくする
- image


* 勉強
   
