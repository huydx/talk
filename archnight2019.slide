秒間数千万メトリクスを裁く監視基盤のアーキテクチャ

huydx
エンジニア/LINE Corp
@dxhuy

* 今日の話
- 弊社のObservability事情
- 時系列データベースを作ったの楽しい話
- 時系列データベースを作ったの辛い話


* LINEのObservabilityチーム

- LINE自体ができる前から作っていたモニタリングプラットフォーム
- 社内全体にプラットフォーム（SASS)として提供している 
- Metrics / Log / Alert / Distributed Tracingなどをたくさん機能を提供する

* Observabilityの三つの柱

.image ./picture/obs1.png 

* 今日はMetrics柱をフォーカスします


* メトリクスプラットフォームを提供するためには

.image ./picture/obs2.png 500 900


* メトリクスストレージ概要

- データポイントを保存する
- レンジを指定してデータポイントをクエリする

- メトリクスストレージ用語

.image ./picture/obs5.png 300 900


* メトリクスストレージ概要
.code ./code/metrics1.go  


* 弊社のメトリクスストレージ歴史
- ~ 2016 Mysqlでめっちゃ頑張った
- 2016 ~ 2018　OpenTSDBに切り替えた
- 2018 OpenTSDBの限界を感じて新しいストレージ開発を着手始めた
- 2019 新しいストレージをリリース

* 僕たちが求められるワークロード

- 数万、数十万ホスト、１ホストあたり1万弱のメトリクスを保存できる
- アラートなどの用途で秒間数せクエリを耐えられる
- 垂直でスケールできる

* なぜオープンソース利用しないの？

.image ./picture/obs6.png 360 900

* なぜDatadog,Mackerelなど利用しないの？
- マイグレーションコスト
- 利用コスト
- カスタマイズ問題（例えば：年末年始のメトリクスは永久保存, 細かいACL制御）

* 結果: ツールの限界を超えて、自分たちで作るのを決めた

* 成果

.image ./picture/obs7.png 270 900


* 本題：どうやって作れる？

* 基本のアイディア
- 機能ごとにマイクロサービスに分散して最適化する
- 直近28時間データを全部メモリにフィットさせる
- できる部分をオープンソースを使って開発コストを下げる

* ざっくりアーキテクチャ

.image ./picture/obs9.png 500 500

* コミュニケーションプロトコルはGrpc
- Grpcは早くはない（marshal/unmarshal）は遅いとメモリアロケーションが爆発
- だが、便利の機能がたくさん揃っている（クライアントサイドLB,自由にカスタマイズできるResolver,リトライなど）

* メモリベースストレージの設計について

* なぜRedis/Memcachedなど使わないの？

- １日莫大データポイント数を保存するためには、一番大事なのは圧縮
- 通常のKVストア（あるいはすでにあるデータ構造）は時系列データを効率的に圧縮するのが難しい
- 自分が欲しいAPI(データを効率的にストリーム）などがない

* 圧縮方法
- １日兆単位のデータポイントがくる
- Facebook Gorilla Delta-Delta XORアルゴリズム（prometheusと同様）
- さらにホット（４時間以内）データを分けて、４時間以上データをさらにLZ4圧縮
- メモリアロケーションを最小限にする（バッファプール）

* メモリベースストレージの難しさ

- スケールアウトするためにステート持つのがだめ
- 再起動したらデータロスはだめ
- 一つのホストが死んだらデータロスもダメ（High Availability)

* スケールアウト問題

- Serie IDベースのデータ分散

.image ./picture/obs10.png 360 500

* 再起動しても大丈夫な設計

- データが来たらとりあえずローカルディスクに保存（Write Ahead Log)
- ディスク保存バックエンドはRocksDB活用

.image ./picture/obs11.png 300 400

* 一つのホスト死んでもデータロスゼロができる設計

- RAFTベースでレプリケーション３のクラスターを構成(シャードと呼ぶ）
- 一つのマシンが死んでも影響なし（残り２台が稼働する）
- 死んだマシンが復旧したらRAFTログが残り２台から自動的にストリムされるので楽


* RAFTベースのクラスター構成 

.image ./picture/obs13.png 500 800

* データの永久保存について

* データの永久保存について
- インメモリストレージがあるため仕事がすごい楽
- Cassandra活用している
- ４時間ごとに直接メモリからバッチサーバでストリーミングする
- ストリーミング単位は時間軸で蓄積された４時間分データなので特にオーダ気にしなくていい
- ダウンサンプリングもすごい楽（データがオーダすでに保つから）

* データの永久保存
.image ./picture/obs14.png 580 400


* 勉強
- ちゃんとスケールする物を作るのには、アーキテクチャが大事
- ストレージ作るのが大変ので特に理由なければオススメはしない
- 今後やるものまだいっぱいあるので、手伝う人が欲しい。。（時系列データベース好きな人はDMしてください）
